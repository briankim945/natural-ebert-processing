{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7287aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nep_loss import multiclass_logloss\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8408b59",
   "metadata": {},
   "source": [
    "# Data Encoding and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bad5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdaa73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931c82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data.nosync/ebert_test_train_data.pkl', 'rb') as f:\n",
    "    train_data, test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9403773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_data, stratify=train_data['stars'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b1f31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5204, 5204, 579, 579, 1928, 1928)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data['review']\n",
    "y_train = train_data['stars']\n",
    "X_valid = valid_data['review']\n",
    "y_valid = valid_data['stars']\n",
    "X_test = test_data['review']\n",
    "y_test = test_data['stars']\n",
    "\n",
    "len(X_train), len(y_train), len(X_valid), len(y_valid), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469ec13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y_encoded = lbl_enc.fit_transform(list(y_train) + list(y_valid) + list(y_test))\n",
    "y_train_enc = y_encoded[0:len(y_train)]\n",
    "y_valid_enc = y_encoded[len(y_train):len(y_train) + len(y_valid)]\n",
    "y_test_enc = y_encoded[len(y_train) + len(y_valid):len(y_train) + len(y_valid) + len(y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92462388",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2762769",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/abhishek/approaching-almost-any-nlp-problem-on-kaggle/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6efaf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775b76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:594: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(X_train) + list(X_valid))\n",
    "xtrain_tfv =  tfv.transform(X_train) \n",
    "xvalid_tfv = tfv.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052a4c2",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0, max_iter=1000)\n",
    "clf.fit(xtrain_tfv, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'w') as f:\n",
    "    f.write(\"Logistic Regression logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43364d2d",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c70b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(X_train) + list(X_valid))\n",
    "xtrain_ctv =  ctv.transform(X_train) \n",
    "xvalid_ctv = ctv.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'a') as f:\n",
    "    f.write(\"Logistic Regression + Word Counts logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "    f.write(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a3570",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd959d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa95627",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'a') as f:\n",
    "    f.write(\"Naive Bayes + TF-IDF logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "    f.write(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19357269",
   "metadata": {},
   "source": [
    "## Naive Bayes + Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd96a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'a') as f:\n",
    "    f.write(\"Naive Bayes + Word Counts logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "    f.write(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b596f",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5487c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'a') as f:\n",
    "    f.write(\"SVM + TF-IDF logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "    f.write(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843de09f",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e24915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90145cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807dfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'a') as f:\n",
    "    f.write(\"XGBoost + TF-IDF logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "    f.write(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840915ab",
   "metadata": {},
   "source": [
    "## XGBoost + SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a121bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores.txt', 'a') as f:\n",
    "    f.write(\"XGBoost + SVD logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "    f.write(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dea4f4",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1b57c",
   "metadata": {},
   "source": [
    "## SVD + Scaling + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94381ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics, pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c15a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD()\n",
    "scl = preprocessing.StandardScaler()\n",
    "lr_model = LogisticRegression()\n",
    "clf = pipeline.Pipeline(\n",
    "    [\n",
    "        ('svd', svd),\n",
    "        ('scl', scl),\n",
    "        ('lr', lr_model)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab02706",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2'],\n",
    "              'lr__max_iter': [100, 1000],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa86e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1,\n",
    "#                      iid=True,\n",
    "                     refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, y_train_enc)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5c79e",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "#                         subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "\n",
    "xbg_model = xgb.XGBClassifier(nthread=-1)\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('xgb', xgb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {\n",
    "    'xgb__max_depth': list(range(10)),\n",
    "    'xgb__n_estimators': [10, 50, 100, 200, 500],\n",
    "    'xgb__colsample_bytree': [0.1, 0.3, 0.5, 0.8, 0.9],\n",
    "    'xgb__subsample': [0.1, 0.3, 0.5, 0.8, 0.9],\n",
    "    'xgb__learning_rate': [0.001, 0.01, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1,\n",
    "#                      iid=True,\n",
    "                     refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, y_train_enc)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b283e",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1,\n",
    "#                      iid=True,\n",
    "                     refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, y_train_enc)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeefaf4",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45eca1c",
   "metadata": {},
   "source": [
    "## Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6223fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54872it [00:02, 19726.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52343\n",
      "['.', '.', '.', '-0.1573', '-0.29517', '0.30453', '-0.54773', '0.098293', '-0.1776', '0.21662']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130611it [00:06, 18150.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128261\n",
      "['at', 'name@domain.com', '0.0061218', '0.39595', '-0.22079', '0.78149', '0.38759', '0.28888', '0.18495', '-0.37328']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153588it [00:08, 18882.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151102\n",
      "['.', '.', '.', '.', '.', '-0.23773', '-0.82788', '0.82326', '-0.91878', '0.35868']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203035it [00:10, 19856.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200668\n",
      "['to', 'name@domain.com', '0.33865', '0.12698', '-0.16885', '0.55476', '0.48296', '0.45018', '0.0094233', '-0.36575']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "212827it [00:11, 18876.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209833\n",
      "['.', '.', '0.035974', '-0.024421', '0.71402', '-0.61127', '0.012771', '-0.11201', '0.16847', '-0.14069']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224593it [00:11, 19533.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220779\n",
      "['.', '.', '.', '.', '0.033459', '-0.085658', '0.27155', '-0.56132', '0.60419', '-0.027276']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255480it [00:13, 18831.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253461\n",
      "['email', 'name@domain.com', '0.33529', '0.32949', '0.2646', '0.64219', '0.70701', '-0.074487', '-0.066128', '-0.30804']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369454it [00:19, 18549.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365745\n",
      "['or', 'name@domain.com', '0.48374', '0.49669', '-0.25089', '0.90389', '0.60307', '0.11141', '-0.021157', '0.10037']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "535418it [00:28, 19235.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532048\n",
      "['contact', 'name@domain.com', '0.016426', '0.13728', '0.18781', '0.75784', '0.44012', '0.096794', '0.060987', '0.31293']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "720716it [00:38, 17539.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717302\n",
      "['Email', 'name@domain.com', '0.37344', '0.024573', '-0.12583', '0.36009', '0.25605', '0.07326', '0.3292', '-0.0037022']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996308it [00:55, 14034.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994818\n",
      "['on', 'name@domain.com', '0.037295', '-0.15381', '-0.045189', '1.0566', '0.42898', '0.24093', '0.34305', '-0.090393']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125347it [01:02, 18387.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1123331\n",
      "['At', 'Killerseats.com', '-0.13854', '-0.01706', '-0.13651', '0.1237', '0.15633', '-0.16556', '0.29374', '-0.064174']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1150408it [01:03, 19301.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148409\n",
      "['by', 'name@domain.com', '0.6882', '-0.36436', '0.62079', '1.1482', '-0.055475', '-0.37936', '0.0064471', '-0.33046']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1355175it [01:15, 18711.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352110\n",
      "['in', 'mylot.com', '-0.18148', '0.47096', '0.32916', '0.044196', '-0.93045', '-0.16299', '0.31996', '0.39017']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1502174it [01:23, 18896.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499727\n",
      "['emailing', 'name@domain.com', '0.39173', '-0.39132', '-0.4266', '0.82429', '0.42919', '0.17601', '0.16663', '-0.011601']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1535688it [01:25, 18071.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1533809\n",
      "['Contact', 'name@domain.com', '0.14933', '-0.28605', '0.3444', '0.29015', '-0.22999', '0.1271', '0.35722', '0.35118']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1902210it [01:44, 19204.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899841\n",
      "['at', 'name@domain.com', '0.44321', '-0.40005', '-0.20065', '1.1209', '0.34041', '0.086082', '-0.067128', '0.0022702']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1924886it [01:46, 18594.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1921152\n",
      "['â€¢', 'name@domain.com', '-0.13288', '-0.31383', '-0.032356', '0.52036', '-0.26985', '0.43339', '0.32587', '-0.51581']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2061247it [01:53, 19397.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058966\n",
      "['at', 'Amazon.com', '-0.5275', '-0.73685', '0.10968', '0.22214', '-0.30063', '-0.63201', '-0.053204', '-0.16241']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2168904it [01:59, 19252.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165246\n",
      "['is', 'name@domain.com', '-0.1197', '0.10706', '-0.10519', '-0.12412', '0.4096', '-0.0287', '0.34704', '0.3549']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:00, 18235.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 words vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('../../glove.840B.300d.txt') as f:\n",
    "    i = 0\n",
    "    for line in tqdm(f):\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except Exception as e:\n",
    "            print(i)\n",
    "            print(values[:10])\n",
    "        i += 1\n",
    "        \n",
    "print(\"Found %s words vectors\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61dbfc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vector(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e0ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = [sentence2vector(x) for x in X_train]\n",
    "xvalid_glove = [sentence2vector(x) for x in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda7a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86c985de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02062d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [21:58:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.881 \n",
      "accuracy: 0.364\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print(\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3645e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [21:58:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.749 \n",
      "accuracy: 0.387\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, y_train_enc)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print(\"logloss: %0.3f \" % multiclass_logloss(y_valid_enc, predictions))\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_valid_enc, np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf47963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
