{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe5a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dc5c9",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa10424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76c32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data.nosync/ebert_test_train_data.pkl', 'rb') as f:\n",
    "    train_data, test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75a4038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5204 5204 579 579 1928 1928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:594: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n",
      "2196017it [02:13, 16500.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 words vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_test_split(train_data, stratify=train_data['stars'], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = train_data['review']\n",
    "y_train = train_data['stars']\n",
    "X_valid = valid_data['review']\n",
    "y_valid = valid_data['stars']\n",
    "X_test = test_data['review']\n",
    "y_test = test_data['stars']\n",
    "\n",
    "print(len(X_train), len(y_train), len(X_valid), len(y_valid), len(X_test), len(y_test))\n",
    "\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y_encoded = lbl_enc.fit_transform(list(y_train) + list(y_valid) + list(y_test))\n",
    "y_train_enc = y_encoded[0:len(y_train)]\n",
    "y_valid_enc = y_encoded[len(y_train):len(y_train) + len(y_valid)]\n",
    "y_test_enc = y_encoded[len(y_train) + len(y_valid):len(y_train) + len(y_valid) + len(y_test)]\n",
    "\n",
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(X_train) + list(X_valid))\n",
    "xtrain_tfv =  tfv.transform(X_train) \n",
    "xvalid_tfv = tfv.transform(X_valid)\n",
    "\n",
    "# from http://www-nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "\n",
    "embeddings_index = {}\n",
    "missed_values = []\n",
    "with open('../../glove.840B.300d.txt') as f:\n",
    "    i = 0\n",
    "    for line in tqdm(f):\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except Exception as e:\n",
    "            missed_values.append(values)\n",
    "        i += 1\n",
    "        \n",
    "print(\"Found %s words vectors\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b621eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vector(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722e13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_glove = [sentence2vector(x) for x in X_train]\n",
    "x_valid_glove = [sentence2vector(x) for x in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acea5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_glove = np.array(xtrain_glove)\n",
    "x_valid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8ba008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0},\n",
       " {0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_data['stars']), set(test_data['stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d32627",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ee58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense, Activation, Dropout, Embedding, BatchNormalization, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc79d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "x_train_glove_scl = scl.fit_transform(x_train_glove)\n",
    "x_valid_glove_scl = scl.fit_transform(y_train_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0e10c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = utils.to_categorical(y_train)\n",
    "y_valid_enc = utils.to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f79bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70ff0c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "82/82 [==============================] - 2s 7ms/step - loss: 1.6545 - val_loss: 1.1853\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 1.1786 - val_loss: 1.1185\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 1.0197 - val_loss: 1.0971\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.9462 - val_loss: 1.0728\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.8577 - val_loss: 1.0756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa2fae7e070>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_glove_scl, y=y_train_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(x_valid_glove_scl, y_valid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd3086",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75347b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e7a143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_valid))\n",
    "x_train_seq = token.texts_to_sequences(X_train)\n",
    "x_valid_seq = token.texts_to_sequences(X_valid)\n",
    "\n",
    "# zero pad the sequences\n",
    "x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\n",
    "x_valid_pad = sequence.pad_sequences(x_valid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "049acd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 85752/85752 [00:00<00:00, 86024.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b1281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37d75859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 9s 587ms/step - loss: 1.3968 - val_loss: 1.3462\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 7s 662ms/step - loss: 1.3359 - val_loss: 1.3213\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 8s 762ms/step - loss: 1.3296 - val_loss: 1.3297\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 8s 721ms/step - loss: 1.3188 - val_loss: 1.3188\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 7s 672ms/step - loss: 1.3182 - val_loss: 1.3122\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 8s 685ms/step - loss: 1.3037 - val_loss: 1.3065\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 8s 685ms/step - loss: 1.3018 - val_loss: 1.3043\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 10s 955ms/step - loss: 1.2939 - val_loss: 1.2910\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 7s 608ms/step - loss: 1.2794 - val_loss: 1.3004\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 7s 628ms/step - loss: 1.2660 - val_loss: 1.2711\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 8s 759ms/step - loss: 1.2587 - val_loss: 1.2707\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 7s 640ms/step - loss: 1.2515 - val_loss: 1.2561\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 7s 657ms/step - loss: 1.2391 - val_loss: 1.2751\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 9s 795ms/step - loss: 1.2381 - val_loss: 1.2667\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 8s 764ms/step - loss: 1.2327 - val_loss: 1.2699\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 10s 857ms/step - loss: 1.2189 - val_loss: 1.2444\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 9s 829ms/step - loss: 1.2225 - val_loss: 1.2641\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 10s 932ms/step - loss: 1.2010 - val_loss: 1.2596\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 9s 756ms/step - loss: 1.1943 - val_loss: 1.2618\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 7s 643ms/step - loss: 1.1902 - val_loss: 1.2655\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 7s 615ms/step - loss: 1.1791 - val_loss: 1.2451\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 7s 611ms/step - loss: 1.1734 - val_loss: 1.2519\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 7s 626ms/step - loss: 1.1681 - val_loss: 1.2889\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 7s 633ms/step - loss: 1.1598 - val_loss: 1.2411\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 7s 631ms/step - loss: 1.1555 - val_loss: 1.2609\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 7s 675ms/step - loss: 1.1389 - val_loss: 1.2849\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 7s 618ms/step - loss: 1.1456 - val_loss: 1.2576\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 7s 617ms/step - loss: 1.1404 - val_loss: 1.2424\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 7s 642ms/step - loss: 1.1371 - val_loss: 1.2490\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 7s 618ms/step - loss: 1.1317 - val_loss: 1.2791\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 7s 596ms/step - loss: 1.1244 - val_loss: 1.2515\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 7s 603ms/step - loss: 1.1151 - val_loss: 1.2522\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 7s 626ms/step - loss: 1.0928 - val_loss: 1.2785\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 7s 597ms/step - loss: 1.1014 - val_loss: 1.2929\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 7s 634ms/step - loss: 1.0915 - val_loss: 1.2740\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 7s 588ms/step - loss: 1.0808 - val_loss: 1.2844\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 7s 625ms/step - loss: 1.0601 - val_loss: 1.2685\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 7s 629ms/step - loss: 1.0522 - val_loss: 1.3196\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 7s 643ms/step - loss: 1.0550 - val_loss: 1.2925\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 7s 588ms/step - loss: 1.0404 - val_loss: 1.3164\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 7s 594ms/step - loss: 1.0284 - val_loss: 1.2918\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 7s 593ms/step - loss: 1.0218 - val_loss: 1.3389\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 7s 608ms/step - loss: 1.0094 - val_loss: 1.3097\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 6s 572ms/step - loss: 1.0152 - val_loss: 1.2884\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 6s 566ms/step - loss: 1.0056 - val_loss: 1.3213\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 7s 591ms/step - loss: 0.9919 - val_loss: 1.3338\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 7s 630ms/step - loss: 0.9747 - val_loss: 1.3288\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 7s 665ms/step - loss: 0.9707 - val_loss: 1.3265\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 7s 621ms/step - loss: 0.9461 - val_loss: 1.3683\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 6s 573ms/step - loss: 0.9405 - val_loss: 1.3881\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 8s 765ms/step - loss: 0.9402 - val_loss: 1.3882\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 7s 662ms/step - loss: 0.9257 - val_loss: 1.3961\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 6s 577ms/step - loss: 0.9241 - val_loss: 1.3573\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 6s 584ms/step - loss: 0.9183 - val_loss: 1.3539\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 6s 577ms/step - loss: 0.9134 - val_loss: 1.4020\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 7s 591ms/step - loss: 0.8965 - val_loss: 1.3984\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 7s 608ms/step - loss: 0.8714 - val_loss: 1.4644\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 7s 600ms/step - loss: 0.8870 - val_loss: 1.4262\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 7s 590ms/step - loss: 0.8593 - val_loss: 1.4832\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 7s 601ms/step - loss: 0.8254 - val_loss: 1.4673\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 6s 575ms/step - loss: 0.8329 - val_loss: 1.5026\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 7s 590ms/step - loss: 0.8201 - val_loss: 1.4875\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 7s 644ms/step - loss: 0.8166 - val_loss: 1.5340\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 7s 614ms/step - loss: 0.8037 - val_loss: 1.5071\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 7s 612ms/step - loss: 0.8087 - val_loss: 1.4915\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 7s 594ms/step - loss: 0.7607 - val_loss: 1.5854\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 7s 607ms/step - loss: 0.7776 - val_loss: 1.5497\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 7s 587ms/step - loss: 0.7613 - val_loss: 1.6319\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 7s 604ms/step - loss: 0.7549 - val_loss: 1.6045\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 6s 581ms/step - loss: 0.7568 - val_loss: 1.6036\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 6s 553ms/step - loss: 0.7328 - val_loss: 1.5961\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 7s 593ms/step - loss: 0.7326 - val_loss: 1.6259\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 7s 588ms/step - loss: 0.7202 - val_loss: 1.6397\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 6s 585ms/step - loss: 0.7098 - val_loss: 1.6940\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 7s 597ms/step - loss: 0.7155 - val_loss: 1.6339\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 7s 662ms/step - loss: 0.6923 - val_loss: 1.7204\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 7s 621ms/step - loss: 0.6929 - val_loss: 1.6961\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 8s 682ms/step - loss: 0.6834 - val_loss: 1.6917\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 7s 621ms/step - loss: 0.6711 - val_loss: 1.7035\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 7s 639ms/step - loss: 0.6623 - val_loss: 1.7247\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 7s 649ms/step - loss: 0.6349 - val_loss: 1.7855\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 7s 618ms/step - loss: 0.6372 - val_loss: 1.7291\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 7s 616ms/step - loss: 0.6645 - val_loss: 1.7804\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 7s 598ms/step - loss: 0.6459 - val_loss: 1.7094\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 7s 630ms/step - loss: 0.6207 - val_loss: 1.8221\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 8s 701ms/step - loss: 0.6419 - val_loss: 1.7403\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 8s 689ms/step - loss: 0.6176 - val_loss: 1.8063\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 8s 707ms/step - loss: 0.6001 - val_loss: 1.8003\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 7s 632ms/step - loss: 0.6169 - val_loss: 1.8297\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 7s 668ms/step - loss: 0.5873 - val_loss: 1.8524\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 7s 620ms/step - loss: 0.5832 - val_loss: 1.8562\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 7s 658ms/step - loss: 0.5783 - val_loss: 1.8630\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 8s 693ms/step - loss: 0.5327 - val_loss: 1.9570\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 7s 628ms/step - loss: 0.5593 - val_loss: 2.0194\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 7s 660ms/step - loss: 0.5730 - val_loss: 1.8657\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 7s 601ms/step - loss: 0.5443 - val_loss: 1.9887\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 8s 697ms/step - loss: 0.5451 - val_loss: 1.9289\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 8s 677ms/step - loss: 0.5478 - val_loss: 1.9353\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 8s 666ms/step - loss: 0.5373 - val_loss: 2.0586\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 8s 712ms/step - loss: 0.5468 - val_loss: 2.0154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa300ed20a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train_pad, \n",
    "    y=y_train_enc, \n",
    "    batch_size=512, \n",
    "    epochs=100, \n",
    "    verbose=1, \n",
    "    validation_data=(x_valid_pad, y_valid_enc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60749192",
   "metadata": {},
   "source": [
    "### Early-Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "640808c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 20s 1s/step - loss: 1.3988 - val_loss: 1.3350\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 17s 2s/step - loss: 1.3415 - val_loss: 1.3447\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.3269 - val_loss: 1.3335\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 18s 2s/step - loss: 1.3176 - val_loss: 1.3033\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.3059 - val_loss: 1.3070\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2829 - val_loss: 1.2886\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2711 - val_loss: 1.2998\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 17s 2s/step - loss: 1.2547 - val_loss: 1.2750\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 14s 1s/step - loss: 1.2523 - val_loss: 1.3098\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2571 - val_loss: 1.2813\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2462 - val_loss: 1.2594\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2208 - val_loss: 1.3023\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.2216 - val_loss: 1.2481\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2151 - val_loss: 1.2611\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2256 - val_loss: 1.2665\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 15s 1s/step - loss: 1.2039 - val_loss: 1.2516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa30e2f80d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(x_valid_pad, y_valid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98a73717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 35s 2s/step - loss: 1.4112 - val_loss: 1.3360\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 28s 2s/step - loss: 1.3528 - val_loss: 1.3368\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 27s 2s/step - loss: 1.3327 - val_loss: 1.3362\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 26s 2s/step - loss: 1.3220 - val_loss: 1.3454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa188603be0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(x_valid_pad, y_valid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0cadccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 33s 2s/step - loss: 1.4081 - val_loss: 1.3450\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 27s 2s/step - loss: 1.3437 - val_loss: 1.3448\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 28s 3s/step - loss: 1.3361 - val_loss: 1.3528\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 26s 2s/step - loss: 1.3255 - val_loss: 1.3625\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 30s 3s/step - loss: 1.3154 - val_loss: 1.3485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa1280dd2b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(x_valid_pad, y_valid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70503f3",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf81759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main ensembling class. how to use it is in the next cell!\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: model dictionary, see README for its format\n",
    "        :param num_folds: the number of folds for ensembling\n",
    "        :param task_type: classification or regression\n",
    "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
    "        :param lower_is_better: is lower value of optimization function better or higher\n",
    "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: training data in tabular format\n",
    "        :param y: binary, multi-class or regression\n",
    "        :return: chain of models to be used in prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "\n",
    "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82c563",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ee1ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cc3deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(X_train) + list(X_valid))\n",
    "xtrain_ctv =  ctv.transform(X_train) \n",
    "xvalid_ctv = ctv.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab0108",
   "metadata": {},
   "source": [
    "## Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d32dd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "from nep_loss import multiclass_logloss\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef120a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:55] INFO Found 9 classes\n",
      "[16:34:55] INFO Training Level 0 Fold # 1. Model # 0\n",
      "[16:35:13] INFO Predicting Level 0. Fold # 1. Model # 0\n",
      "[16:35:14] INFO Level 0. Fold # 1. Model # 0. Validation Score = 1.732681\n",
      "[16:35:14] INFO Training Level 0 Fold # 2. Model # 0\n",
      "[16:35:33] INFO Predicting Level 0. Fold # 2. Model # 0\n",
      "[16:35:33] INFO Level 0. Fold # 2. Model # 0. Validation Score = 1.726837\n",
      "[16:35:33] INFO Training Level 0 Fold # 3. Model # 0\n",
      "[16:35:49] INFO Predicting Level 0. Fold # 3. Model # 0\n",
      "[16:35:49] INFO Level 0. Fold # 3. Model # 0. Validation Score = 1.727804\n",
      "[16:35:49] INFO Level 0. Model # 0. Mean Score = 1.729107. Std Dev = 0.002558\n",
      "[16:35:49] INFO Training Level 0 Fold # 1. Model # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:43:59] INFO Predicting Level 0. Fold # 1. Model # 1\n",
      "[16:43:59] INFO Level 0. Fold # 1. Model # 1. Validation Score = 1.844908\n",
      "[16:43:59] INFO Training Level 0 Fold # 2. Model # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:52:40] INFO Predicting Level 0. Fold # 2. Model # 1\n",
      "[16:52:40] INFO Level 0. Fold # 2. Model # 1. Validation Score = 1.814251\n",
      "[16:52:40] INFO Training Level 0 Fold # 3. Model # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:44] INFO Predicting Level 0. Fold # 3. Model # 1\n",
      "[17:00:45] INFO Level 0. Fold # 3. Model # 1. Validation Score = 1.841146\n",
      "[17:00:45] INFO Level 0. Model # 1. Mean Score = 1.833435. Std Dev = 0.013652\n",
      "[17:00:45] INFO Training Level 0 Fold # 1. Model # 2\n",
      "[17:00:45] INFO Predicting Level 0. Fold # 1. Model # 2\n",
      "[17:00:45] INFO Level 0. Fold # 1. Model # 2. Validation Score = 2.475651\n",
      "[17:00:45] INFO Training Level 0 Fold # 2. Model # 2\n",
      "[17:00:45] INFO Predicting Level 0. Fold # 2. Model # 2\n",
      "[17:00:45] INFO Level 0. Fold # 2. Model # 2. Validation Score = 2.457662\n",
      "[17:00:45] INFO Training Level 0 Fold # 3. Model # 2\n",
      "[17:00:45] INFO Predicting Level 0. Fold # 3. Model # 2\n",
      "[17:00:45] INFO Level 0. Fold # 3. Model # 2. Validation Score = 2.465671\n",
      "[17:00:45] INFO Level 0. Model # 2. Mean Score = 2.466328. Std Dev = 0.007359\n",
      "[17:00:45] INFO Training Level 0 Fold # 1. Model # 3\n",
      "[17:00:47] INFO Predicting Level 0. Fold # 1. Model # 3\n",
      "[17:00:47] INFO Level 0. Fold # 1. Model # 3. Validation Score = 22.848331\n",
      "[17:00:47] INFO Training Level 0 Fold # 2. Model # 3\n",
      "[17:00:48] INFO Predicting Level 0. Fold # 2. Model # 3\n",
      "[17:00:48] INFO Level 0. Fold # 2. Model # 3. Validation Score = 22.782568\n",
      "[17:00:48] INFO Training Level 0 Fold # 3. Model # 3\n",
      "[17:00:49] INFO Predicting Level 0. Fold # 3. Model # 3\n",
      "[17:00:49] INFO Level 0. Fold # 3. Model # 3. Validation Score = 22.877138\n",
      "[17:00:49] INFO Level 0. Model # 3. Mean Score = 22.836012. Std Dev = 0.039579\n",
      "[17:00:49] INFO Saving predictions for level # 0\n",
      "[17:00:50] INFO Training Level 1 Fold # 1. Model # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [17:00:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:56] INFO Predicting Level 1. Fold # 1. Model # 0\n",
      "[17:00:56] INFO Level 1. Fold # 1. Model # 0. Validation Score = 2.056281\n",
      "[17:00:56] INFO Training Level 1 Fold # 2. Model # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [17:00:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:01] INFO Predicting Level 1. Fold # 2. Model # 0\n",
      "[17:01:01] INFO Level 1. Fold # 2. Model # 0. Validation Score = 1.974962\n",
      "[17:01:01] INFO Training Level 1 Fold # 3. Model # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [17:01:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:01:06] INFO Predicting Level 1. Fold # 3. Model # 0\n",
      "[17:01:06] INFO Level 1. Fold # 3. Model # 0. Validation Score = 1.891059\n",
      "[17:01:06] INFO Level 1. Model # 0. Mean Score = 1.974101. Std Dev = 0.067454\n",
      "[17:01:06] INFO Saving predictions for level # 1\n",
      "[17:01:06] INFO Training Fulldata Level 0. Model # 0\n",
      "[17:01:34] INFO Predicting Test Level 0. Model # 0\n",
      "[17:01:34] INFO Training Fulldata Level 0. Model # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briankim/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "\n",
    "ens.fit(train_data_dict, y_train, lentrain=x_train_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=x_valid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6369e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check error:\n",
    "multiclass_logloss(y_valid, preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b481a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, preds[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
